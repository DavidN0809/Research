{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72067b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "# Define the duration of the recording in seconds\n",
    "duration = 2\n",
    "\n",
    "# Set the sampling frequency and number of channels\n",
    "fs = 44100\n",
    "channels = 1\n",
    "\n",
    "# Start recording\n",
    "print('Recording started...')\n",
    "recording = sd.rec(int(duration * fs), samplerate=fs, channels=channels)\n",
    "\n",
    "# Wait for the recording to finish\n",
    "sd.wait()\n",
    "\n",
    "# Save the recording to a wave file\n",
    "filename = 'recording.wav'\n",
    "wav.write(filename, fs, recording)\n",
    "\n",
    "# Play the audio back out\n",
    "#print('Playing back...')\n",
    "#playback = wav.read(filename)\n",
    "#sd.play(playback[1], playback[0])\n",
    "#sd.wait()\n",
    "#print('Playback finished.')\n",
    "\n",
    "block_len_ms = 32 \n",
    "block_shift_ms = 8\n",
    "fs_target = 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf636f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-23 18:13:43.434787: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-23 18:13:43.436714: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-23 18:13:43.465875: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-23 18:13:43.466255: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-23 18:13:44.044331: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calc fft\n",
      "reshape\n",
      "(1, 1, 257)\n",
      "set tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import tensorflow.lite as tflite\n",
    "\n",
    "# set some parameters\n",
    "block_len_ms = 32 \n",
    "block_shift_ms = 8\n",
    "fs_target = 16000\n",
    "\n",
    "# create the interpreters\n",
    "interpreter_1 = tflite.Interpreter(model_path='./pretrained_model/model_1.tflite')\n",
    "interpreter_1.allocate_tensors()\n",
    "interpreter_2 = tflite.Interpreter(model_path='./pretrained_model/model_2.tflite')\n",
    "interpreter_2.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details_1 = interpreter_1.get_input_details()\n",
    "output_details_1 = interpreter_1.get_output_details()\n",
    "input_details_2 = interpreter_2.get_input_details()\n",
    "output_details_2 = interpreter_2.get_output_details()\n",
    "\n",
    "# create states for the lstms\n",
    "states_1 = np.zeros(input_details_1[1]['shape']).astype('float32')\n",
    "states_2 = np.zeros(input_details_2[1]['shape']).astype('float32')\n",
    "\n",
    "# calculate shift and length\n",
    "block_shift = int(np.round(fs_target * (block_shift_ms / 1000)))\n",
    "block_len = int(np.round(fs_target * (block_len_ms / 1000)))\n",
    "\n",
    "# create buffer\n",
    "in_buffer = np.zeros((block_len)).astype('float32')\n",
    "out_buffer = np.zeros((block_len)).astype('float32')\n",
    "\n",
    "\n",
    "# calculate fft of input block\n",
    "in_block_fft = np.fft.rfft(in_buffer)\n",
    "in_mag = np.abs(in_block_fft)\n",
    "in_phase = np.angle(in_block_fft)\n",
    "print('calc fft')\n",
    "\n",
    "# reshape magnitude to input dimensions\n",
    "in_mag = np.reshape(in_mag, (1,1,-1)).astype('float32')\n",
    "print('reshape')\n",
    "print(in_mag.shape)\n",
    "    \n",
    "interpreter_1.set_tensor(input_details_1[1]['index'], states_1)\n",
    "interpreter_1.set_tensor(input_details_1[0]['index'], in_mag)\n",
    "print('set tensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3abe1de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input device: 23\n",
      "Using output device: 23\n",
      "################################################################################\n",
      "Press Ctrl+C to stop\n",
      "################################################################################\n",
      "\n",
      "Closing stream\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "# set some parameters\n",
    "block_len_ms = 32 \n",
    "block_shift_ms = 8\n",
    "fs_input = 44100\n",
    "fs_target = 16000\n",
    "\n",
    "# create the interpreters\n",
    "interpreter_1 = tflite.Interpreter(model_path='./pretrained_model/model_1.tflite')\n",
    "interpreter_1.allocate_tensors()\n",
    "interpreter_2 = tflite.Interpreter(model_path='./pretrained_model/model_2.tflite')\n",
    "interpreter_2.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details_1 = interpreter_1.get_input_details()\n",
    "output_details_1 = interpreter_1.get_output_details()\n",
    "input_details_2 = interpreter_2.get_input_details()\n",
    "output_details_2 = interpreter_2.get_output_details()\n",
    "\n",
    "# create states for the lstms\n",
    "states_1 = np.zeros(input_details_1[1]['shape']).astype('float32')\n",
    "states_2 = np.zeros(input_details_2[1]['shape']).astype('float32')\n",
    "\n",
    "# calculate shift and length\n",
    "block_shift = int(np.round(fs_target * (block_shift_ms / 1000)))\n",
    "block_len = int(np.round(fs_target * (block_len_ms / 1000)))\n",
    "\n",
    "# create buffer\n",
    "in_buffer = np.zeros((block_len)).astype('float32')\n",
    "out_buffer = np.zeros((block_len)).astype('float32')\n",
    "\n",
    "# resample audio to the target sampling rate\n",
    "def resample(audio, sr, target_sr):\n",
    "    resampled = librosa.resample(audio, sr, target_sr)\n",
    "    return resampled, target_sr\n",
    "\n",
    "def callback(indata, outdata, frames, time, status):\n",
    "    print('starting callback')\n",
    "    # buffer and states to global\n",
    "    global in_buffer, out_buffer, states_1, states_2\n",
    "    if status:\n",
    "        print(status)\n",
    "    print('passing states')\n",
    "    \n",
    "    # resample audio to the target sampling rate\n",
    "    indata, fs_input = resample(indata, fs_input, fs_target)\n",
    "        \n",
    "    # write to buffer\n",
    "    in_buffer[:-block_shift] = in_buffer[block_shift:]\n",
    "    in_buffer[-block_shift:] = np.squeeze(indata)\n",
    "    print('writing to buffer')\n",
    "\n",
    "    # save a copy of the input audio\n",
    "#    sf.write('input_audio.wav', indata, fs_target, subtype='PCM_24')\n",
    "#    print('saving input wav')\n",
    "    \n",
    "    # calculate fft of input block\n",
    "    in_block_fft = np.fft.rfft(in_buffer)\n",
    "    in_mag = np.abs(in_block_fft)\n",
    "    in_phase = np.angle(in_block_fft)\n",
    "    print('calc fft')\n",
    "\n",
    "    # reshape magnitude to input dimensions\n",
    "    in_mag = np.reshape(in_mag, (1,1,-1)).astype('float32')\n",
    "    print('reshape')\n",
    "    print(in_mag.shape)\n",
    "\n",
    "    # set tensors to the first model\n",
    "    interpreter_1.set_tensor(input_details_1[1]['index'], states_1)\n",
    "    interpreter_1.set_tensor(input_details_1[0]['index'], in_mag)\n",
    "    print('set tensors')\n",
    "\n",
    "    # run calculation \n",
    "    interpreter_1.invoke()\n",
    "    print('pass invoke')\n",
    "\n",
    "    # get the output of the first block\n",
    "    out_mask = interpreter_1.get_tensor(output_details_1[0]['index']) \n",
    "    states_1 = interpreter_1.get_tensor(output_details_1[1]['index'])   \n",
    "    print('output first block')\n",
    "    \n",
    "    # calculate the ifft\n",
    "    estimated_complex = in_mag * out_mask * np.exp(1j * in_phase)\n",
    "    estimated_complex = np.concatenate([estimated_complex, np.flip(np.conj(estimated_complex[1:-1]), axis=0)], axis=0)\n",
    "    estimated_block = np.fft.irfft(estimated_complex)\n",
    "    print('calc ifft')\n",
    "\n",
    "    # downsample the estimated block\n",
    "    downsampled_block = estimated_block[::int(fs_target/16000)]\n",
    "\n",
    "    # reshape the time domain block\n",
    "    downsampled_block = np.reshape(downsampled_block, (1,1,-1)).astype('float32')\n",
    "    print('reshape td block')\n",
    "\n",
    "    # set tensors to the second block\n",
    "    interpreter_2.set_tensor(input_details_2[1]['index'], states_2)\n",
    "    interpreter_2.set_tensor(input_details_2[0]['index'], downsampled_block)\n",
    "    print('tensors of second block')\n",
    "\n",
    "    # run calculation\n",
    "    interpreter_2.invoke()  \n",
    "    print('int 2 invoke')\n",
    "\n",
    "    # get output tensors\n",
    "    out_block = interpreter_2.get_tensor(output_details_2[0]['index']) \n",
    "    states_2 = interpreter_2.get_tensor(output_details_2[1]['index'])\n",
    "\n",
    "    # upsample the output block\n",
    "    upsampled_block = np.zeros((block_len))\n",
    "    upsampled_block[::int(fs_target/16000)] = np.squeeze(out_block)\n",
    "\n",
    "    # write to buffer\n",
    "    out_buffer[:-block_shift] = out_buffer[block_shift:]\n",
    "    out_buffer[-block_shift:] = upsampled_block\n",
    "    print('writting to outbuffer')\n",
    "\n",
    "    # output to soundcard\n",
    "    outdata[:] = np.expand_dims(out_buffer[:block_shift], axis=-1)\n",
    "\n",
    "#choose the default input and output devices\n",
    "input_device = 23\n",
    "output_device = 23\n",
    "\n",
    "#print the chosen devices\n",
    "print(f\"Using input device: {input_device}\")\n",
    "print(f\"Using output device: {output_device}\")\n",
    "\n",
    "#open a sound stream with the chosen devices and the callback function\n",
    "with sd.Stream(device=(input_device, output_device),\n",
    "    samplerate=fs_target, blocksize=block_shift,\n",
    "    dtype=np.float32, channels=1, callback=callback):\n",
    "    print('#' * 80)\n",
    "    print('Press Ctrl+C to stop')\n",
    "    print('#' * 80)\n",
    "    \n",
    "while True:\n",
    "    try:\n",
    "        sd.sleep(100)\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nClosing stream')\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(type(e).name + ': ' + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ef2ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [  1   1 257]\n",
      "Output shape: [  1   1 257]\n",
      "pass invoke\n"
     ]
    }
   ],
   "source": [
    "interpreter_1 = tflite.Interpreter(model_path='./pretrained_model/model_1.tflite')\n",
    "interpreter_1.allocate_tensors()\n",
    "\n",
    "input_details = interpreter_1.get_input_details()\n",
    "output_details = interpreter_1.get_output_details()\n",
    "\n",
    "print(\"Input shape:\", input_details[0]['shape'])\n",
    "print(\"Output shape:\", output_details[0]['shape'])\n",
    "\n",
    "interpreter_1.invoke()\n",
    "print(\"pass invoke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3da3d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio in real-time...\n",
      "block shape: (128, 1)\n",
      "elapsed_time: 0.023009777069091797\n",
      "block shape: (128, 1)\n",
      "elapsed_time: 0.02329707145690918\n",
      "block shape: (128, 1)\n",
      "elapsed_time: 0.06627702713012695\n",
      "block shape: (128, 1)\n",
      "No audio recorded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12654/3964938109.py:62: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if np.any(block == b'q') or elapsed_time >= duration:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Dimension mismatch. Got 2 but expected 3 for input 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m input_details \u001b[38;5;241m=\u001b[39m interpreter_1\u001b[38;5;241m.\u001b[39mget_input_details()\n\u001b[1;32m     43\u001b[0m output_details \u001b[38;5;241m=\u001b[39m interpreter_1\u001b[38;5;241m.\u001b[39mget_output_details()\n\u001b[0;32m---> 44\u001b[0m \u001b[43minterpreter_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m interpreter_1\u001b[38;5;241m.\u001b[39minvoke()\n\u001b[1;32m     46\u001b[0m output_1 \u001b[38;5;241m=\u001b[39m interpreter_1\u001b[38;5;241m.\u001b[39mget_tensor(output_details[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/lite/python/interpreter.py:696\u001b[0m, in \u001b[0;36mInterpreter.set_tensor\u001b[0;34m(self, tensor_index, value)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_tensor\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_index, value):\n\u001b[1;32m    681\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sets the value of the input tensor.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m  Note this copies data in `value`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m    ValueError: If the interpreter could not set the tensor.\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 696\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSetTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Dimension mismatch. Got 2 but expected 3 for input 0."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import scipy.io.wavfile as wav\n",
    "import time\n",
    "import tensorflow.lite as tflite\n",
    "\n",
    "# Define the window size and shift\n",
    "fs = 16000  # Set the sampling frequency and number of channels\n",
    "channels = 1\n",
    "window_size = int(32 * fs / 1000)  # 32ms window size\n",
    "shift_size = int(8 * fs / 1000)  # 8ms shift\n",
    "\n",
    "# Create the interpreters\n",
    "interpreter_1 = tflite.Interpreter(model_path='./pretrained_model/model_1.tflite')\n",
    "interpreter_1.allocate_tensors()\n",
    "interpreter_2 = tflite.Interpreter(model_path='./pretrained_model/model_2.tflite')\n",
    "interpreter_2.allocate_tensors()\n",
    "\n",
    "# Start processing the audio in real-time\n",
    "print('Processing audio in real-time...')\n",
    "stream = sd.Stream(samplerate=fs, blocksize=shift_size, channels=channels)\n",
    "\n",
    "# Start the stream\n",
    "stream.start()\n",
    "\n",
    "# Initialize the audio buffer and window\n",
    "audio_buffer = []\n",
    "window = np.array([])\n",
    "\n",
    "duration = 5  # Set the recording duration to 5 seconds\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Read a block of audio samples\n",
    "        block, overflow = stream.read(shift_size)\n",
    "        print(\"block shape:\", block.shape)\n",
    "\n",
    "        # Extract the current window and perform inference on model 1\n",
    "        window = np.concatenate((window, block[:, 0]), axis=0)[-window_size:]\n",
    "        if len(window) == window_size:\n",
    "            input_details = interpreter_1.get_input_details()\n",
    "            output_details = interpreter_1.get_output_details()\n",
    "            interpreter_1.set_tensor(input_details[0]['index'], np.expand_dims(window.astype(np.float32), axis=0))\n",
    "            interpreter_1.invoke()\n",
    "            output_1 = interpreter_1.get_tensor(output_details[0]['index'])\n",
    "\n",
    "            # Perform inference on model 2 using the output of model 1\n",
    "            input_details = interpreter_2.get_input_details()\n",
    "            output_details = interpreter_2.get_output_details()\n",
    "            interpreter_2.set_tensor(input_details[0]['index'], output_1)\n",
    "            interpreter_2.invoke()\n",
    "            output_2 = interpreter_2.get_tensor(output_details[0]['index'])\n",
    "\n",
    "            # Append the output of model 2 to the audio buffer\n",
    "            audio_buffer.append(output_2)\n",
    "            print(\"output_2 shape:\", output_2.shape)\n",
    "\n",
    "        # Check if the \"q\" key was pressed or the duration has elapsed\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(\"elapsed_time:\", elapsed_time)\n",
    "        if np.any(block == b'q') or elapsed_time >= duration:\n",
    "            if np.any(block == b'q'):\n",
    "                print('\\nRecording stopped with \"q\" key...')\n",
    "            else:\n",
    "                print(f'\\nRecording stopped after {duration} seconds...')\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Stop the stream when Ctrl-C is pressed\n",
    "    stream.stop()\n",
    "    print('\\nStream stopped.')\n",
    "\n",
    "finally:\n",
    "    # Save the audio buffer to a wave file\n",
    "    if len(audio_buffer) > 0:\n",
    "        filename = 'my_recording.wav'\n",
    "        audio_data = np.concatenate(audio_buffer, axis=0)\n",
    "        wav.write(filename, fs, audio_data)\n",
    "        print(f'Audio saved to \"{filename}\" with shape:', audio_data.shape)\n",
    "    else:\n",
    "        print('No audio recorded')  # Add a message if no audio is recorded\n",
    "\n",
    "    # Stop the stream\n",
    "    stream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b909eda5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
